{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Time of execution for Colab free.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports.**\n",
    "\n",
    "**Data parsing.**\n",
    "\n",
    "**Preprocess.**\n",
    "\n",
    "    Data cleaning.\n",
    "    Tokenization.\n",
    "    Vectorizing.\n",
    "    \n",
    "**Cosim similarity (Clastering).**\n",
    "    \n",
    "**Machine Learning.**\n",
    "\n",
    "**Bonus. Word2Vec.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check environment\"\"\"\n",
    "!pwd\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-BXJrZa4c0d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Install & import libraries for NLP and more\"\"\"\n",
    "\n",
    "#! pip install -U textblob\n",
    "#! pip install --upgrade scikit-learn\n",
    "#! pip install nltk\n",
    "#! pip install gensim\n",
    "#! pip install tensorflow\n",
    "\n",
    "\"\"\"Install the package is capable of resolving contractions (and slang): contractions\"\"\"\n",
    "#! pip install contractions\n",
    "\n",
    "\"\"\"Install Open source library for Emoticons and Emoji detection library: emot\"\"\"\n",
    "#! pip install emot\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Deeplearning library import, spetial solution\"\"\"\n",
    "\n",
    "import tensorflow\n",
    "# TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1676146693552,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "dhoRvyhymbZg"
   },
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JTnOfVF4d06"
   },
   "source": [
    "**Data parsing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1676146700132,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "88tt9lWA44Wd"
   },
   "outputs": [],
   "source": [
    "\"\"\"Function for read files, marking(0 or 1) and join dataframes then.\"\"\"\n",
    "\n",
    "def get_data():\n",
    "    df_pos = pd.read_csv('./data/processedPositive.csv').transpose().reset_index()\n",
    "    df_pos['tone'] = 1\n",
    "    df_neg = pd.read_csv('./data/processedNegative.csv').transpose().reset_index()\n",
    "    df_neg['tone'] = -1\n",
    "    df_neu = pd.read_csv('./data/processedNeutral.csv').transpose().reset_index()\n",
    "    df_neu['tone'] = 0\n",
    "    df_data = pd.concat([df_pos, df_neg, df_neu])\n",
    "    df_data_save = df_data.copy()\n",
    "    return df_data, df_data_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4073,
     "status": "ok",
     "timestamp": 1676146708062,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "KqPrGGKqB0WC",
    "outputId": "2fc8270e-c35c-41d5-d80e-42f16dcabcab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data, df_data_save = get_data()\n",
    "print(df_data.head())\n",
    "print(df_data.describe(), '\\n')\n",
    "print(type(df_data))\n",
    "\n",
    "# display(df_data_save[16:3000:500])\n",
    "# print(df_data_save.describe(), '\\n')\n",
    "# print(type(df_data_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1676146711371,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "AFYRim1aoD1i",
    "outputId": "743d1485-73db-48bc-d0bb-b92cd898a087"
   },
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0VM5x71kpAB"
   },
   "source": [
    "**Preprocess:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFT_n-ybnlTu"
   },
   "source": [
    "**Data cleaning.**\n",
    "\n",
    "    Contraction to full form\n",
    "    Replace emoticons with text\n",
    "\n",
    "    Remove: \n",
    "          ticks and next symbol\n",
    "          url (http*)\n",
    "          hashtags (#)\n",
    "          mentions (@)\n",
    "          stopwords\n",
    "          extra spaces\n",
    "          punctuation\n",
    "\n",
    "    Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1676146717216,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "5muYkExO7o22",
    "outputId": "9c014465-7966-477f-c6c5-8e7dd917bcab"
   },
   "outputs": [],
   "source": [
    "#   !! WALKING AROUND !!\n",
    "\"\"\"Find all mentions in Dataframe\"\"\"\n",
    "\n",
    "text = \"Beautiful album greatest unsung guitar genius @PeterPen @BobbySquash\"\n",
    "list_of_mentions = []\n",
    "for txt in df_data['index']:\n",
    "    mentions = (re.findall(\"@[A-Za-z0-9_]{1,50}\", txt))\n",
    "    if mentions:\n",
    "        list_of_mentions+=(mentions)\n",
    "mentions = (re.findall(\"@[A-Za-z0-9_]{1,50}\", text))\n",
    "if mentions:\n",
    "      list_of_mentions+=(mentions)\n",
    "    # list_of_mentions+=re.findall(\"@[A-Za-z0-9_]{1,50}\", text)\n",
    "print('1) list_of_mentions:',list_of_mentions)\n",
    "\n",
    "#   Find all hashtags in Dataframe\n",
    "list_of_hashtags = []\n",
    "text = \"Beautiful album greatest unsung guitar genius #KingPing #DingDongKong\"\n",
    "for txt in df_data['index']:\n",
    "    hashtags = (re.findall(\"#[A-Za-z0-9_]{1,50}\", txt))\n",
    "    if hashtags:\n",
    "        list_of_hashtags+=(hashtags)\n",
    "hashtags = (re.findall(\"#[A-Za-z0-9_]{1,50}\", text))\n",
    "if hashtags:\n",
    "      list_of_hashtags+=(hashtags)\n",
    "    # list_of_hashtags+=re.findall(\"#[A-Za-z0-9_]{1,50}\", text)\n",
    "print('2) list_of_hashtags:', list_of_hashtags)\n",
    "\n",
    "#   !! WALKING AROUND !!\n",
    "\"\"\"Example is cleaning Dataframe from URLs with re.exp.\"\"\"\n",
    "text = \"This text has or had URL https://profile.intra.42.fr/# inside!\"\n",
    "clear = re.sub('http\\S+','',text)\n",
    "print('1) clear:', clear)\n",
    "\n",
    "\"\"\"Example is cleaning Dataframe from URLs with urlparse and list compehension\"\"\"\n",
    "from urllib.parse import urlparse\n",
    "text = \"This text has or had URLs https://profile.intra.42.fr/# and https://21-school.ru/ inside!\"\n",
    "not_url_list = [item for item in text.split() if not urlparse(item).scheme]\n",
    "clear = ' '.join(not_url_list)\n",
    "print('2) clear:', clear)\n",
    "\n",
    "#   !! WALKING AROUND !!\n",
    "\"\"\"Find URLs in Dataframe\"\"\"\n",
    "def find_url(text, url_lst):\n",
    "    urls = (re.findall('http\\S+', text))\n",
    "    if urls:\n",
    "        url_lst = url_lst+urls\n",
    "    return url_lst\n",
    "\n",
    "url_lst = []\n",
    "url_lst = df_data['index'].apply(lambda x: find_url(x, url_lst))\n",
    "for url in url_lst:\n",
    "      if url:\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1676146733105,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "S5p3WrZoXHf0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1620,
     "status": "ok",
     "timestamp": 1676146736656,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "fRMkHWIsX6It",
    "outputId": "ad213cae-aa04-492a-de60-28c837309f0d"
   },
   "outputs": [],
   "source": [
    "df_data, df_data_save = get_data()      # get data and make copy of DataFrame\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function fix contractions\"\"\"\n",
    "\n",
    "import contractions\n",
    "df_data['index'] = df_data['index'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function replace emoticons with text\"\"\"\n",
    "\n",
    "def emoticon_transform(text, emot):\n",
    "    emoticons = emot.emoticons(text).get('value')\n",
    "    means = emot.emoticons(text).get('mean')\n",
    "    if emoticons:\n",
    "        for i, emot in enumerate(emoticons):\n",
    "            text = text.replace(emot, ' ' + means[i] + ' ')\n",
    "    return text\n",
    "\n",
    "import emot\n",
    "\n",
    "emot = emot.core.emot()\n",
    "df_data['index'] = df_data['index'].apply(lambda x: emoticon_transform(x, emot))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is cleaning Dataframe from english stopwords\"\"\"\n",
    "\n",
    "def drop_stopwords(text, en_stopwords):\n",
    "    # print(en_stopwords)\n",
    "    tokens_lst = []\n",
    "    for token in text.split():\n",
    "        if token.lower() not in en_stopwords:\n",
    "            tokens_lst.append(token)\n",
    "\n",
    "    return ' '.join(tokens_lst)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = stopwords.words('english')\n",
    "df_data['index'] = df_data['index'].apply(lambda x: drop_stopwords(x, en_stopwords))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is cleaning Dataframe from hashtags and mentions\"\"\"\n",
    "\n",
    "def drop_hashtags_and_mentions(text):\n",
    "    clean_text = re.sub(\"@[A-Za-z0-9_]+\",\"\",text) # replace mention with empty string\n",
    "    clean_text = re.sub(\"#[a-zA-Z0-9_]+\",\"\",clean_text) # replace hashtag with empty string\n",
    "    return clean_text\n",
    "\n",
    "df_data['index'] = df_data['index'].apply(lambda x: drop_hashtags_and_mentions(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is cleaning Dataframe from URLs\"\"\"\n",
    "\n",
    "def drop_url(text):\n",
    "    return re.sub('http\\S+','',text)\n",
    "\n",
    "df_data['index'] = df_data['index'].apply(lambda x: drop_url(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is cleaning Dataframe from ticks \n",
    "    and the next character (do it when contractions has fixed only)\"\"\"\n",
    "\n",
    "def drop_ticks_and_nextone(text):\n",
    "    return re.sub(r\"\\'\\w+\",'',text)\n",
    "\n",
    "df_data['index'] = df_data['index'].apply(lambda x: drop_ticks_and_nextone(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is cleaning Dataframe from numbers\"\"\"\n",
    "\n",
    "def drop_numbers(text):\n",
    "    return re.sub(r\"\\d+\", ' ',text)\n",
    "\n",
    "df_data['index'] = df_data['index'].apply(lambda x: drop_numbers(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is cleaning Dataframe from punctuations\"\"\"\n",
    "\n",
    "def drop_punctuations(text):\n",
    "    return re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "df_data['index'] = df_data['index'].apply(lambda x: drop_punctuations(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is converting Dataframe to lower case\"\"\"\n",
    "\n",
    "df_data['index'] = df_data['index'].apply(lambda x: x.lower())\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is cleaning Dataframe from numbers\"\"\"\n",
    "\n",
    "def drop_over_spaces(text):\n",
    "    text = ' '.join(text.split())\n",
    "    return re.sub(r\"\\s{2,}\", ' ',text)\n",
    "\n",
    "df_data['index'] = df_data['index'].apply(lambda x: drop_over_spaces(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is makeing Dataframe with unique rows\"\"\"\n",
    "\n",
    "df_data = df_data.drop_duplicates(subset='index')\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Function is droping empty rows\"\"\"\n",
    "\n",
    "df_data = df_data.replace(r'', float('NaN'), regex=True).dropna()\n",
    "\n",
    "print(df_data[25:3025:525])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBi6Qfdv_5cv"
   },
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATpBBUndEKc-"
   },
   "source": [
    "**Tokenization.**\n",
    "\n",
    "    Just tokenization\n",
    "    Stemming (2 ways)\n",
    "    Lematization\n",
    "    Stemming + misspelling\n",
    "    Lematization + misspelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "executionInfo": {
     "elapsed": 5176,
     "status": "ok",
     "timestamp": 1676146752210,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "1mUFUtZRvcD_",
    "outputId": "1832e1cf-acde-4005-ea0b-36eac7513ca5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "\"\"\"Just tokenize (by sentence) - deiding text to text units\"\"\"\n",
    "\n",
    "def tokenization(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return (word_tokenize(text))\n",
    "\n",
    "df_tokenized = df_data.copy()\n",
    "df_tokenized['processed'] = df_tokenized['index'].apply(lambda x: tokenization(x))\n",
    "display('1. df_tokenized:', df_tokenized.head())\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Stemming (by word) - removing word endings (PorterStemmer)\"\"\"\n",
    "\n",
    "def stemming(text, stemmer):\n",
    "    text = text.split()\n",
    "    stem_lst = list()\n",
    "    for word in text:\n",
    "        stem = stemmer.stem(word)\n",
    "        stem_lst.append(stem)\n",
    "    return (' '.join(stem_lst))\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "df_stemmed = df_data.copy()\n",
    "df_stemmed['processed'] = (df_stemmed['index']\n",
    "                                            .apply(lambda x: stemming(x, stemmer))\n",
    "                                            .apply(lambda x: tokenization(x))) \n",
    "display(' --------------------------------------------- ','2. df_stemmed:', df_stemmed.head())\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Stemming (by word) - removing word endings (LancasterStemmer)\"\"\"\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer_lc = LancasterStemmer()\n",
    "df_stemmed_lc = df_data.copy()\n",
    "df_stemmed_lc['processed'] = (df_stemmed_lc['index']\n",
    "                                            .apply(lambda x: stemming(x, stemmer_lc))\n",
    "                                            .apply(lambda x: tokenization(x)))\n",
    "display(' --------------------------------------------- ','3. df_stemmed_lc:',df_stemmed_lc.head())\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Lemmatization (by word) - converting to normal form\"\"\"\n",
    "\n",
    "def lemmatization(text, lemmatizer):\n",
    "    text = text.split()\n",
    "    lemm_lst = list()\n",
    "    for word in text:\n",
    "        lemm = lemmatizer.lemmatize(word)\n",
    "        lemm_lst.append(lemm)\n",
    "    return (' '.join(lemm_lst))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_lemmatized = df_data.copy()\n",
    "df_lemmatized['processed'] = (df_lemmatized['index']\n",
    "                                            .apply(lambda x: lemmatization(x, lemmatizer))\n",
    "                                            .apply(lambda x: tokenization(x))) \n",
    "display(' --------------------------------------------- ','4. df_lemmatized:', df_lemmatized.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 410998,
     "status": "ok",
     "timestamp": 1676147172298,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "QMu-JjkQEkeg",
    "outputId": "783a0fb0-bab7-48a3-fc8e-5e66c4c355fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "\"\"\"Fix misspelings with TextBlob (~7 minutes)\"\"\"\n",
    "\n",
    "def correct_spelling(text):\n",
    "    from textblob import TextBlob\n",
    "    text = TextBlob(text)\n",
    "    return str(text.correct())\n",
    "\n",
    "df_misspelling_off = df_data.copy()\n",
    "df_misspelling_off['processed'] = df_misspelling_off['index'].apply(lambda x: correct_spelling(x))\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Stemming (by word) - removing word endings (PorterStemmer) + df_misspelling_off\"\"\"\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "df_stemmed_plus = df_misspelling_off.copy()\n",
    "df_stemmed_plus['processed'] = (df_stemmed['index']\n",
    "                                            .apply(lambda x: stemming(x, stemmer))\n",
    "                                            .apply(lambda x: tokenization(x))) \n",
    "display(' --------------------------------------------- ','5. df_stemmed_plus :',df_stemmed_plus)\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Stemming (by word) - removing word endings (LancasterStemmer) + df_misspelling_off\"\"\"\n",
    "\n",
    "stemmer_lc = LancasterStemmer()\n",
    "\n",
    "df_stemmed_lc_plus = df_data.copy()\n",
    "df_stemmed_lc_plus['processed'] = (df_stemmed['index']\n",
    "                                            .apply(lambda x: stemming(x, stemmer))\n",
    "                                            .apply(lambda x: tokenization(x)))\n",
    "display(' --------------------------------------------- ','6. df_stemmed_lc_plus:',df_stemmed_lc_plus.head())\n",
    "\n",
    "####################################################################################################\n",
    "\"\"\"Lemmatization (by word) + df_misspelling_off\"\"\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_lemmatized_plus = df_misspelling_off.copy() \n",
    "df_lemmatized_plus['processed'] = (df_lemmatized['index']\n",
    "                                            .apply(lambda x: lemmatization(x, lemmatizer))\n",
    "                                            .apply(lambda x: tokenization(x))) \n",
    "display(' --------------------------------------------- ','7. df_lemmatized_plus :',df_lemmatized_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTgpvWqyeIeo"
   },
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5HogFG1vLxl"
   },
   "source": [
    "**Vectorizing**:\n",
    "\n",
    "    Word embedding:\n",
    "        One-Hot-Encoding\n",
    "        Bag of words\n",
    "        TF-IDF\n",
    "    \n",
    "        Word2vec.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "error",
     "timestamp": 1676148140623,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "Tl8odi2TgIFS",
    "outputId": "70762807-b4e7-4e6e-ab99-3910b2c856c8"
   },
   "outputs": [],
   "source": [
    "##   !! WALKING AROUND !!\n",
    "\"\"\"One-Hot-Encoding (SLOW experiment)\"\"\"\n",
    "\n",
    "one_hot_encoding = pd.DataFrame(index=corpus)\n",
    "for document in corpus:\n",
    "    for token in document.split():\n",
    "        if token not in one_hot_encoding.columns:\n",
    "            one_hot_encoding[token] = 0\n",
    "\n",
    "def mark(row):\n",
    "    for word, mark in row.iteritems():\n",
    "        if word in row.name.split():\n",
    "            row[word] = 1\n",
    "    return row\n",
    "\n",
    "one_hot_encoding = one_hot_encoding.apply(lambda row: mark(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11798,
     "status": "ok",
     "timestamp": 1676148168132,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "4-ycq9VzLRxt"
   },
   "outputs": [],
   "source": [
    "\"\"\"Some ways to vectorize\"\"\"\n",
    "\n",
    "def vectorize(df_for_vectorizing):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "    \n",
    "    corpus = df_for_vectorizing['processed'].apply(lambda row: ' '.join(row))\n",
    "    index_s = df_for_vectorizing['index']\n",
    "\n",
    "    \"\"\"Word count: Bag-of-Words \n",
    "    — это статистический анализ, анализирующий количественное вхождение слов в документах.\"\"\"\n",
    "    bow_vectorizer = CountVectorizer()\n",
    "    bow_matrix = bow_vectorizer.fit_transform(corpus).toarray()\n",
    "    bag_of_words = pd.DataFrame(data=bow_matrix, \n",
    "                                index=index_s, \n",
    "                                columns=bow_vectorizer.get_feature_names_out())\n",
    "    \"\"\"One-Hot-Encoding (0 or 1, if the word exist)\n",
    "    - быстрая разметка\"\"\"\n",
    "    one_hot_encoding = bag_of_words.copy()\n",
    "    for column in one_hot_encoding.columns:\n",
    "        one_hot_encoding[column] = (one_hot_encoding[column]\n",
    "                                    .apply(lambda item: item if not item else 1))\n",
    "\n",
    "    \"\"\"TF-IDF \n",
    "    (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера,\n",
    "    используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса.\n",
    "    Вес некоторого слова пропорционален частоте употребления этого слова в документе\n",
    "    и обратно пропорционален частоте употребления слова во всех документах коллекции.\"\"\"\n",
    "\n",
    "    tf_idf_vectorizer = TfidfVectorizer()\n",
    "    tf_idf_matrix = tf_idf_vectorizer.fit_transform(corpus).todense()\n",
    "    tf_idf = pd.DataFrame(data=tf_idf_matrix, \n",
    "                        index = index_s,\n",
    "                        columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return one_hot_encoding, bag_of_words, tf_idf\n",
    "\n",
    "one_hot_encoding, bag_of_words, tf_idf = vectorize(df_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1676148168133,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "R6NRyVDwNJZn",
    "outputId": "89956bca-4f06-4995-d2bb-61bda0d7f04d"
   },
   "outputs": [],
   "source": [
    "one_hot_encoding[one_hot_encoding['zoo']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBn-1B2egxK_"
   },
   "source": [
    "**Cosim similarity (Clastering):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 126504,
     "status": "ok",
     "timestamp": 1676148305242,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "Bibr7X2aWK2a",
    "outputId": "9dc03c43-9e01-46ab-b4a2-a1ca29f1cada"
   },
   "outputs": [],
   "source": [
    "\"\"\"Find 10 most similar tweets one for each (~12 min)\n",
    "    return TOP 10 for each\"\"\"\n",
    "\n",
    "top_counter = 0     # Count result tables\n",
    "\n",
    "tokenizers = { \n",
    "                'just_tokenization':df_tokenized,\n",
    "                'stemming Porter': df_stemmed,\n",
    "                'lematization': df_lemmatized,\n",
    "                'stemming Lancaster': df_stemmed_lc,\n",
    "                'stemming Porter + misspelling': df_stemmed_plus,\n",
    "                'stemming Lancaster + misspelling': df_stemmed_lc_plus,\n",
    "                'lematization + misspelling': df_lemmatized_plus,\n",
    "\n",
    "                }\n",
    "\n",
    "for tokenizer_key in tokenizers:\n",
    "    print('TOKENIZER -', tokenizer_key)\n",
    "    df_tokens = tokenizers.get(tokenizer_key)\n",
    "    one_hot_encoding, bag_of_words, tf_idf = vectorize(df_tokens) # here we make vectors\n",
    "    vectorizers = {\n",
    "                    '0 or 1, if the word exist':one_hot_encoding,\n",
    "                    'word count':bag_of_words,\n",
    "                    'tf-idf':tf_idf\n",
    "                    }\n",
    "    for vectoriser_key in vectorizers:\n",
    "        print('#', top_counter)\n",
    "        print('Vectorizer -', vectoriser_key)\n",
    "        df_vectors = vectorizers.get(vectoriser_key)\n",
    "\n",
    "        \"\"\"Making matrix of cosine_semilarity for dataset's vectors\"\"\"\n",
    "        from scipy.spatial import distance\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        cos_sim = cosine_similarity(df_vectors.values) # here we make calculation\n",
    "        df_similarity = pd.DataFrame(cos_sim, columns=df_vectors.index.values, index=df_vectors.index) # here we make matrix\n",
    "\n",
    "        \"\"\"Make TOP-10 similar tweets DataFrame\"\"\"\n",
    "        top = pd.DataFrame(columns = ['tweet_x','tweet_y','cosine_similarity'])\n",
    "\n",
    "        def new_top_candidat(X,Y,rate):\n",
    "            \"\"\"function make new row for appending to top dataframe\"\"\"\n",
    "            rate_data = {'tweet_x':[X],'tweet_y':[Y],'cosine_similarity':[rate]}\n",
    "            df_top_candidat = pd.DataFrame(rate_data)\n",
    "            return df_top_candidat\n",
    "\n",
    "        for Y, row in df_similarity.iterrows():\n",
    "            if Y == 'omg he kissed crying joy':                                                     # this is key-tweet\n",
    "                for X, rate in row.iteritems():\n",
    "                    if X != Y and rate > 0 and rate < 1:                                            # rules for filtering      \n",
    "                        if top.shape[0] < 10:\n",
    "                            df_top_candidat = new_top_candidat(X,Y,rate) \n",
    "                            top = (top.append(df_top_candidat)\n",
    "                                    .sort_values(by='cosine_similarity', ascending=False))\n",
    "                            del(df_top_candidat)\n",
    "                        else:\n",
    "                            min_rate = top.cosine_similarity.min()\n",
    "                            if min_rate < rate:\n",
    "                                top = top.head(9)                                                   # get just top-9\n",
    "                                df_top_candidat = new_top_candidat(X,Y,rate)                        # make 10th for adding\n",
    "                                top = (top.append(df_top_candidat)\n",
    "                                        .sort_values(by='cosine_similarity', ascending=False)       # sort descending by one column\n",
    "                                        .drop_duplicates(subset='cosine_similarity'))               # drop duplicates by one column\n",
    "                                del(df_top_candidat)\n",
    "\n",
    "        if top.shape[0] == 10:\n",
    "            top = top.set_index(pd.Index(range(1,11)))\n",
    "        top_counter+=1\n",
    "        display(top)\n",
    "        # print('Tweet X:\\n',top.tweet_x.values, '\\n')\n",
    "        # print('Tweet Y:\\n',top.tweet_y.values)\n",
    "display('TOTAL TABLES =', top_counter)\n",
    "top.cosine_similarity.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15697,
     "status": "ok",
     "timestamp": 1676148964505,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "k7s2Oim5XTDK",
    "outputId": "91b09a5f-de5f-4e3f-e78d-eea7617845ec"
   },
   "outputs": [],
   "source": [
    "##   !! WALKING AROUND !!\n",
    "\n",
    "one_hot_encoding, bag_of_words, tf_idf = vectorize(df_tokens)                                       # here we make vector\n",
    "df_similarity = pd.DataFrame(cos_sim, columns=tf_idf.index.values, index=tf_idf.index)              # here we make matrix\n",
    "for Y, row in df_similarity.iterrows():                                                             \n",
    "    print(Y)                                                                                        # here we print all tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1676148977032,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "B9h28hLxyXZe",
    "outputId": "1610d17c-b2b7-4a88-c636-44351227fb58"
   },
   "outputs": [],
   "source": [
    "##   !! WALKING AROUND !!\n",
    "\"\"\"Making TOP with different tweets\"\"\"\n",
    "\n",
    "top = pd.DataFrame(columns = ['tweet_x','tweet_y','cosine_similarity'])\n",
    "\n",
    "def new_top_candidat(X,Y,rate):\n",
    "    \"\"\"function make new row for appending to top dataframe\"\"\"\n",
    "    rate_data = {'tweet_x':[X],'tweet_y':[Y],'cosine_similarity':[rate]}\n",
    "    df_top_candidat = pd.DataFrame(rate_data)\n",
    "    return df_top_candidat\n",
    "\n",
    "for Y, row in df_similarity.iterrows():\n",
    "    \"\"\"Uncomment text or write some new\"\"\"\n",
    "    if Y == 'indian heft pope':\n",
    "    # if Y == 'omg he kissed crying joy':\n",
    "    # if Y == 'britain becoming xenophobic hostile':\n",
    "    # if Y == 'supreme court restores bangalore trial court verdict holding others corruption case':\n",
    "        for X, rate in row.iteritems():\n",
    "            if X != Y and rate > 0 and rate < 1: # Rules for filtering        \n",
    "                if top.shape[0] < 10:\n",
    "                    df_top_candidat = new_top_candidat(X,Y,rate) \n",
    "                    top = (top.append(df_top_candidat)\n",
    "                            .sort_values(by='cosine_similarity', ascending=False))\n",
    "                    del(df_top_candidat)\n",
    "                else:\n",
    "                    min_rate = top.cosine_similarity.min()\n",
    "                    if min_rate < rate:\n",
    "                        top = top.head(9)\n",
    "                        df_top_candidat = new_top_candidat(X,Y,rate)\n",
    "                        top = (top.append(df_top_candidat)\n",
    "                                .sort_values(by='cosine_similarity', ascending=False) # sort descending by one column\n",
    "                                .drop_duplicates(subset='cosine_similarity')) # drop duplicates by one column\n",
    "                        del(df_top_candidat)\n",
    "if top.shape[0] == 10:\n",
    "    top = top.set_index(pd.Index(range(1,11)))\n",
    "    \n",
    "display(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILNiRuaPN8ly"
   },
   "outputs": [],
   "source": [
    "##   !! WALKING AROUND !!\n",
    "\"\"\"Find 10 most similar tweets all for each (not very interesting, but maximum similarity rates)\"\"\"\n",
    "\n",
    "tokenizers = { \n",
    "                # 'just_tokenization':df_tokenized,\n",
    "                # 'stemming Porter': df_stemmed,\n",
    "                # 'lematization': df_lemmatized,\n",
    "                # 'stemming Lancaster': df_stemmed_lc,\n",
    "                # 'stemming Porter + misspelling': df_stemmed_plus,\n",
    "                'lematization + misspelling': df_lemmatized_plus,\n",
    "                # 'stemming Lancaster + misspelling': df_stemmed_plus,\n",
    "                }\n",
    "\n",
    "for tokenizer_key in tokenizers:\n",
    "    print('TOKENIZER -', tokenizer_key)\n",
    "    df_tokens = tokenizers.get(tokenizer_key)\n",
    "    one_hot_encoding, bag_of_words, tf_idf = vectorize(df_tokens) # here we make vectors\n",
    "    vectorizers = {\n",
    "                    '0 or 1, if the word exist':one_hot_encoding,\n",
    "                    'word count':bag_of_words,\n",
    "                    'tf-idf':tf_idf\n",
    "                    }\n",
    "    for vectoriser_key in vectorizers:\n",
    "        print('Vectorizer -', vectoriser_key)\n",
    "        df_vectors = vectorizers.get(vectoriser_key)\n",
    "\n",
    "        \"\"\"Making matrix of cosine_semilarity for dataset's vectors\"\"\"\n",
    "        from scipy.spatial import distance\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        cos_sim = cosine_similarity(df_vectors.values) # here we make calculation\n",
    "        df_similarity = pd.DataFrame(cos_sim, columns=df_vectors.index.values, index=df_vectors.index) # here we make matrix\n",
    "\n",
    "        \"\"\"Make TOP-10 similar tweets DataFrame\"\"\"\n",
    "        top = pd.DataFrame(columns = ['tweet_x','tweet_y','cosine_similarity'])\n",
    "\n",
    "        def new_top_candidat(X,Y,rate):\n",
    "            \"\"\"function make new row for appending to top dataframe\"\"\"\n",
    "            rate_data = {'tweet_x':[X],'tweet_y':[Y],'cosine_similarity':[rate]}\n",
    "            df_top_candidat = pd.DataFrame(rate_data)\n",
    "            return df_top_candidat\n",
    "\n",
    "        for Y, row in df_similarity.iterrows():\n",
    "            for X, rate in row.iteritems():\n",
    "                if X != Y and rate > 0 and rate < 1: # Rules for filtering        \n",
    "                    if top.shape[0] < 10:\n",
    "                        df_top_candidat = new_top_candidat(X,Y,rate) \n",
    "                        top = (top.append(df_top_candidat)\n",
    "                                .sort_values(by='cosine_similarity', ascending=False))\n",
    "                        del(df_top_candidat)\n",
    "                    else:\n",
    "                        min_rate = top.cosine_similarity.min()\n",
    "                        if min_rate < rate:\n",
    "                            top = top.head(9)\n",
    "                            df_top_candidat = new_top_candidat(X,Y,rate)\n",
    "                            top = (top.append(df_top_candidat)\n",
    "                                    .sort_values(by='cosine_similarity', ascending=False) # sort descending by one column\n",
    "                                    .drop_duplicates(subset='cosine_similarity')) # drop duplicates by one column\n",
    "                            del(df_top_candidat)\n",
    "\n",
    "        top = top.set_index(pd.Index(range(1,11)))\n",
    "        display(top)\n",
    "        # print('Tweet X:\\n',top.tweet_x.values, '\\n')\n",
    "        # print('Tweet Y:\\n',top.tweet_y.values)\n",
    "top.cosine_similarity.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fn5F6HApJyWW"
   },
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hca6nAFB9Lrk"
   },
   "source": [
    "**Machine Learning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1670816084971,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "oZkdgKkVTjno",
    "outputId": "79509be9-8988-4185-81e0-d89ff45cb1ba"
   },
   "outputs": [],
   "source": [
    "##   !! WALKING AROUND !!\n",
    "\"\"\"Creating DataFrame and adding one row with custom index\"\"\"\n",
    "\n",
    "df_res = pd.DataFrame(columns = ['0 or 1, if the word exist', 'word_count', 'TFIDF'])\n",
    "df_res = df_result.append(pd.DataFrame(data={ '0 or 1, if the word exist':[2222], 'word_count':[1111], 'TFIDF':[3333]}, index=['way']))\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1760892,
     "status": "ok",
     "timestamp": 1670868788733,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "vmnWrIdS9Kkm",
    "outputId": "f52d32f5-cf9d-4b4a-c88e-bfe4eb02dddc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Learning some ML moodels (~42 min)\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tokenizers = { \n",
    "                'just_tokenization':df_tokenized,\n",
    "                'stemming Porter': df_stemmed,\n",
    "                'stemming Lancaster': df_stemmed_lc,\n",
    "                'lematization': df_lemmatized,\n",
    "                'stemming Porter + misspelling': df_stemmed_plus,\n",
    "                'stemming Lancaster + misspelling': df_stemmed_plus,\n",
    "                'lematization + misspelling': df_lemmatized_plus,\n",
    "             }\n",
    "\n",
    "df_result = pd.DataFrame(columns = ['0 or 1, if the word exist', 'word_count', 'TFIDF'])            # create results table\n",
    "for tokenizer_key in tokenizers:\n",
    "    print('\\n', 'TOKENIZER -', tokenizer_key)\n",
    "    df_tokens = tokenizers.get(tokenizer_key)                                                       # get next processor\n",
    "    one_hot_encoding, bag_of_words, tf_idf = vectorize(df_tokens)                                   # get vectors for processed data\n",
    "    vectorizers = { \n",
    "                    'One-Hot-Encoding':one_hot_encoding,\n",
    "                    'Bag Of Words':bag_of_words,\n",
    "                    'TF-IDF':tf_idf,\n",
    "                  }\n",
    "    result_list = list()\n",
    "    for vectoriser_key in vectorizers:\n",
    "        scores = pd.Series([], dtype='object')\n",
    "        df_vector = vectorizers.get(vectoriser_key)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_vector, df_data.tone, test_size=0.2, random_state=42)\n",
    "        print('\\n', 'Preprocessing way:', vectoriser_key)\n",
    "        \"\"\"X - preprocessed data, y - data marks\"\"\"\n",
    "        print('Train dataset part size:',X_train.shape, y_train.shape) \n",
    "        print('Test dataset part size:',X_test.shape, y_test.shape)\n",
    "        print('============')\n",
    "\n",
    "        \"\"\"start learning models\"\"\"\n",
    "\n",
    "        \"\"\"Гауссовский наивный байесовский реализует гауссовский наивный байесовский алгоритм для классификации. \n",
    "        Предполагается, что вероятность появления признаков гауссова\"\"\"\n",
    "        clf = GaussianNB()         \n",
    "        clf.fit(X_train, y_train)\n",
    "        scores = scores.append(pd.Series([clf.score(X_test, y_test)]))\n",
    "        print('Gausian Native Bayes accuracy -',clf.score(X_test, y_test))\n",
    "\n",
    "        \"\"\"Логистическая регрессия — статистическая модель, используемая для прогнозирования \n",
    "        вероятности возникновения некоторого события путём его сравнения с логистической кривой. \n",
    "        Эта регрессия выдаёт ответ в виде вероятности бинарного события (1 или 0).\"\"\"\n",
    "        clf = LogisticRegression(random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores = scores.append(pd.Series([clf.score(X_test, y_test)]))\n",
    "        print('Logistic Regression accuracy -',clf.score(X_test, y_test))\n",
    "\n",
    "        \"\"\"Random Forest — это множество решающих деревьев. \n",
    "        В задаче регрессии их ответы усредняются,\n",
    "        в задаче классификации принимается решение голосованием по большинству.\"\"\"\n",
    "        clf = RandomForestClassifier(max_depth=2, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores = scores.append(pd.Series([clf.score(X_test, y_test)]))\n",
    "        print('Random Forest accuracy -',clf.score(X_test, y_test))\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators = 1025, criterion='entropy', random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(pd.Series([clf.score(X_test, y_test)]))\n",
    "        print('Random Forest with entropy accuracy -',clf.score(X_test, y_test))\n",
    "\n",
    "        \"\"\"Деревья решений (DT) — это непараметрический контролируемый метод обучения, \n",
    "        используемый для классификации и регрессии. \n",
    "        Цель состоит в том, чтобы создать модель, которая предсказывает значение целевой переменной, \n",
    "        изучая простые правила принятия решений, выведенные из характеристик данных. \n",
    "        Дерево можно рассматривать как кусочно-постоянное приближение.\"\"\"\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(pd.Series([clf.score(X_test, y_test)]))\n",
    "        print('Decision Tree accuracy -',clf.score(X_test, y_test))\n",
    "\n",
    "        \"\"\"Метод опорных векторов (англ. SVM, support vector machine) — набор схожих алгоритмов обучения с учителем, \n",
    "        использующихся для задач классификации и регрессионного анализа. \n",
    "        Принадлежит семейству линейных классификаторов и может также рассматриваться как частный случай регуляризации по Тихонову. \n",
    "        Особым свойством метода опорных векторов является непрерывное уменьшение эмпирической ошибки классификации и увеличение зазора, \n",
    "        поэтому метод также известен как метод классификатора с максимальным зазором.\"\"\"\n",
    "        clf = SVC(kernel='linear', C=1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(pd.Series([clf.score(X_test, y_test),]))\n",
    "        print('C-Support Vector Classification accuracy -',clf.score(X_test, y_test))\n",
    "\n",
    "        result_list.append(scores.max())                                                            # results for different BOW to one tokenizer\n",
    "    df_result = (df_result.append(\n",
    "                                  pd.DataFrame(np.array([result_list]), \n",
    "                                               columns=['0 or 1, if the word exist', 'word_count', 'TFIDF'], \n",
    "                                               index=[tokenizer_key]))\n",
    "                                 )                                                                  # add result row \n",
    "\n",
    "                                                                                  \n",
    "\n",
    "\"\"\"Do gridsearch to find the best parameters for ML algorithm (SVC)\"\"\"\n",
    "\n",
    "def do_gridsearch(alg, X_train, y_train):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    parameters = [{\n",
    "            'kernel': ['linear', 'poly', 'rbf'],\n",
    "            'C': [1,25],\n",
    "            'max_iter': [10,100]\n",
    "            }]\n",
    "\n",
    "    clf = GridSearchCV(alg, parameters, scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('GridSearch accuracy params -',clf.best_params_)\n",
    "    print('-------------')\n",
    "\n",
    "alg = SVC()\n",
    "do_gridsearch(alg, X_train, y_train)                                                                # start gridsearch\n",
    "\n",
    "display(df_result)                                                                                  # results of ML accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##   !! WALKING AROUND !!\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "def do_gridsearch(alg, X_train, y_train):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    parameters = [{\n",
    "            'kernel': ['linear', 'poly', 'rbf'],\n",
    "            'C': [1,125],\n",
    "            'max_iter': [10,100]\n",
    "            }]\n",
    "\n",
    "    clf = GridSearchCV(alg, parameters, scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('GridSearch accuracy params -',clf.best_params_)\n",
    "    print('-------------')\n",
    "\n",
    "alg = SVC()\n",
    "do_gridsearch(alg, X_train, y_train)                                                                     # start gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpNXhyGw53F8"
   },
   "source": [
    "**Bonus. Word2Vec.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7625,
     "status": "ok",
     "timestamp": 1670879698362,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "A3xZvmdepY65",
    "outputId": "ad61239d-6c3d-44c1-8bf7-8443566b43b2"
   },
   "outputs": [],
   "source": [
    "\"\"\"! GENSIM 4.0 !\"\"\"\n",
    "\n",
    "corpus = df_data['index'] #df_tokenized_plus['processed'].apply(lambda row: ' '.join(row))\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\"\"\"Vectorizing with Keras libruary\"\"\"\n",
    "\n",
    "tweets_lists = [text_to_word_sequence(tweet) for tweet in corpus]\n",
    "tweets_lists = tweets_lists\n",
    "\n",
    "\n",
    "\"\"\"Initialazing Word2Vec model\"\"\"\n",
    "w2v_model = Word2Vec(sentences=tweets_lists, window=5, min_count=1)\n",
    "\n",
    "\"\"\"First training Word2Vec model\"\"\"\n",
    "w2v_model.train(tweets_lists, total_examples=len(tweets_lists), epochs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1670879797491,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "U8DP13dP8Qne",
    "outputId": "e4b48522-407a-4cc5-b6fa-d28e15ae6db9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Some useful from Word2Vec\"\"\"\n",
    "\n",
    "all_words = list(w2v_model.wv.index_to_key)\n",
    "print('\\n', all_words,'\\n')\n",
    "\n",
    "# display('Word vector:', w2v_model.wv.word_vec('happy'))\n",
    "display('Word vector:', w2v_model.wv.get_vector('happy'))\n",
    "print('\\n')\n",
    "\n",
    "WORD = 'happy'\n",
    "display(f\"Most similar words to word '{WORD}':\", w2v_model.wv.most_similar(WORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3135,
     "status": "ok",
     "timestamp": 1670880000630,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "RGwaUIfrVHu5",
    "outputId": "0d4e2f6a-3335-4c69-c5fe-49944d388ff7",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_word_embeddings(x, vector))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(embeddings)\n\u001b[0;32m---> 25\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mSeries(tweets_lists),w2v_model)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"X - preprocessed data, y - data marks\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(embeddings, df_data\u001b[38;5;241m.\u001b[39mtone, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Machine learning with Word2Vec\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\"\"\"Making embeddings\"\"\"\n",
    "def get_embeddings(tokens,vector):\n",
    "    def get_word_embeddings(token_word_list,vector,k=100):\n",
    "        if len(token_word_list) < 1:\n",
    "            return np.zeros(k)\n",
    "        else:\n",
    "            vectorized = ([vector.wv.get_vector(word) \n",
    "                           if word in vector.wv.index_to_key \n",
    "                           else np.random.rand(k) \n",
    "                           for word in token_word_list]) \n",
    "\n",
    "        summ = np.sum(vectorized,axis=0)\n",
    "        ## return the average\n",
    "        return summ/len(vectorized)\n",
    "\n",
    "    display(tokens)\n",
    "    embeddings = tokens.apply(lambda x: get_word_embeddings(x, vector))\n",
    "    return list(embeddings)\n",
    "\n",
    "embeddings = get_embeddings(pd.Series(tweets_lists),w2v_model)\n",
    "\n",
    "\n",
    "\"\"\"X - preprocessed data, y - data marks\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df_data.tone, test_size=0.2, random_state=42) \n",
    "# print('\\n', 'Train dataset part:',X_train[:2], y_train[:2]) \n",
    "# print('\\n', 'Test dataset part:',X_test[:2], y_test[:2])\n",
    "# print('||||||||||||')\n",
    "\n",
    "\"\"\"start learning models\"\"\"\n",
    "\n",
    "word2vec = LogisticRegression(solver='lbfgs', max_iter=42)\n",
    "word2vec.fit(X_train, y_train)\n",
    "print('\\n', 'Logistic Regression accuracy with Word2Vec -', word2vec.score(X_test, y_test), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37376,
     "status": "ok",
     "timestamp": 1670879893903,
     "user": {
      "displayName": "Artem T",
      "userId": "05585379007897921772"
     },
     "user_tz": -180
    },
    "id": "dNEUyJUg8nIr",
    "outputId": "40a011c4-52f8-4ec6-bbf2-35025f7c315f"
   },
   "outputs": [],
   "source": [
    "\"\"\"More training Word2Vec model (~1 min).\n",
    "    Need just one more to get 15% better acuracy.\"\"\"\n",
    "\n",
    "w2v_model.train(tweets_lists, total_examples=len(tweets_lists), epochs=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynoi9qZzftBd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4-60qIAJ2hX4"
   },
   "outputs": [],
   "source": [
    "!git add *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sXppAkstSIaJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mnew file:   data/processedNegative.csv\u001b[m\n",
      "\t\u001b[32mnew file:   data/processedNeutral.csv\u001b[m\n",
      "\t\u001b[32mnew file:   data/processedPositive.csv\u001b[m\n",
      "\t\u001b[32mnew file:   tweets.ipynb\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main b3739f1] Ready to get better\r\n",
      " 4 files changed, 1650 insertions(+)\r\n",
      " create mode 100644 data/processedNegative.csv\r\n",
      " create mode 100644 data/processedNeutral.csv\r\n",
      " create mode 100644 data/processedPositive.csv\r\n",
      " create mode 100644 tweets.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Ready to get better\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 8, done.\n",
      "Counting objects: 100% (8/8), done.\n",
      "Delta compression using up to 5 threads\n",
      "Compressing objects: 100% (7/7), done.\n",
      "Writing objects: 100% (7/7), 84.98 KiB | 3.15 MiB/s, done.\n",
      "Total 7 (delta 0), reused 0 (delta 0)\n",
      "To github.com:Dr-Code-lab/NLP_analyze.git\n",
      "   3a1d8f1..b3739f1  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPl71Eb1NlmpwauTf11VLTU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
